{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as sm\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from collections.abc import Iterable, Callable\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CostFunction(ABC):\n",
    "    \"\"\"Abstract class for cost functions\"\"\"\n",
    "    def __init__(self, metrics: Iterable[str], M: 'np.ndarray[float]') -> None:\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            metrics (Iterable[str]): Iterable of strings of the form (metric_name).\n",
    "            M (np.ndarray[float]): Positive definite matrix of size len(metrics).\n",
    "\n",
    "        Raises:\n",
    "            ValueError: _description_\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        self.metrics = metrics\n",
    "        self.M = M or np.identity(len(metrics))  # type: ignore\n",
    "        self._check_positive_definite(self.M)\n",
    "    \n",
    "    @abstractmethod\n",
    "    def functional(self, y_true: 'np.ndarray[float]', y_pred: 'np.ndarray[float]') -> float:\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            y_true (np.ndarray[float]): Array-like of true labels of length N.\n",
    "            y_pred (np.ndarray[float]): Array-like of predicted labels of length N.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def _to_array(y: Iterable[float]) -> 'np.ndarray[float]':\n",
    "        return np.fromiter(y, float)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _check_positive_definite(M: 'np.ndarray[float]') -> None:\n",
    "        if not np.all(np.linalg.eigvals(M) > 0):\n",
    "            raise ValueError(f'Matrix {M} is not positive definite')\n",
    "\n",
    "    def make_scorer(self) -> Callable:\n",
    "        return sm.make_scorer(self.functional, greater_is_better=False)\n",
    "\n",
    "    def __call__(self, y_true: Iterable[float], y_pred: Iterable[float]) -> float:\n",
    "        y_pred_array = self._to_array(y_pred)\n",
    "        y_true_array = self._to_array(y_true)\n",
    "            \n",
    "        return self.functional(y_true_array, y_pred_array)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationCostFunction(CostFunction):\n",
    "    def __init__(self, metrics: Iterable[str], M: 'np.ndarray[float]' = None, metric_class_opt_val_map: dict[str, tuple[str, float]]=None, proba_threshold: float = 0.5):\n",
    "        \"\"\"Defines cost functional for optimization of multiple metrics. \n",
    "        Since this is defined as a loss function, cross validation returns the negative of the score [1].\n",
    "\n",
    "        Args:\n",
    "            metrics (Iterable[str]): Iterable of strings of the form (metric_name).\n",
    "            M (np.ndarray[float]): Positive definite matrix of size len(metrics).\n",
    "            metric_class_map (dict[str, str], optional): Dictionary mapping metric to class or probability of the form {'metric': 'class' or 'proba'}. Defaults to {}.\n",
    "            proba_threshold (float, optional): Probability threshold used to convert probabilities into classes. Defaults to 0.5.\n",
    "            \n",
    "        References:\n",
    "            [1] https://github.com/scikit-learn/scikit-learn/issues/2439\n",
    "            \n",
    "        Example:\n",
    "            >>> y_true = [0, 0, 0, 1, 1]\n",
    "            >>> y_pred = [0.46, 0.6, 0.29, 0.25, 0.012]\n",
    "            >>> threshold = 0.5\n",
    "            >>> metrics = [\"f1_score\", \"roc_auc_score\"]\n",
    "            >>> cf = ClassificationCostFunction(metrics)\n",
    "            >>> np.isclose(cf(y_true, y_pred), 1.41, rtol=1e-01, atol=1e-01)\n",
    "            True\n",
    "            >>> X, y = make_classification()\n",
    "            >>> model = LogisticRegression()\n",
    "            >>> model.fit(X, y)\n",
    "            >>> y_proba = model.predict_proba(X)[:, 1]\n",
    "            >>> cost = cf(y, y_proba)\n",
    "            >>> f1 = getattr(sm, \"f1_score\")\n",
    "            >>> roc_auc = getattr(sm, \"roc_auc_score\")\n",
    "            >>> y_pred = np.where(y_proba > 0.5, 1, 0)\n",
    "            >>> scorer_output = np.sqrt((f1(y, y_pred) - 1.0)**2 + (roc_auc(y, y_proba) - 1.0)**2)\n",
    "            >>> np.isclose(cost, scorer_output)\n",
    "            True\n",
    "        \"\"\"\n",
    "        super().__init__(metrics, M)\n",
    "        self.proba_threshold = proba_threshold\n",
    "        self.metric_class_opt_val_map = metric_class_opt_val_map or {\n",
    "            \"accuracy_score\": (\"class\", 1),\n",
    "            \"f1_score\": (\"class\", 1),\n",
    "            \"log_loss\": (\"class\", 0),\n",
    "            \"precision_score\": (\"class\", 1),\n",
    "            \"recall_score\": (\"class\", 1),\n",
    "            \"roc_auc_score\": (\"proba\", 1),\n",
    "        }\n",
    "        \n",
    "    def _to_class(self, array: 'np.ndarray[float]', metric: str) -> 'np.ndarray[float]':\n",
    "        # sourcery skip: inline-immediately-returned-variable\n",
    "        output = np.where(array > self.proba_threshold, 1, 0) if self.metric_class_opt_val_map[metric][0] == \"class\" else array\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def functional(self, y_true: 'np.ndarray[float]', y_pred: 'np.ndarray[float]') -> float:\n",
    "        \n",
    "        self._check_positive_definite(self.M)\n",
    "\n",
    "        opt_values = np.array([self.metric_class_opt_val_map[metric][1] for metric in self.metrics])\n",
    "\n",
    "        metric_values = np.array([getattr(sm, metric)(y_true, self._to_class(y_pred, metric)) for metric in self.metrics])\n",
    "\n",
    "        return np.sqrt(np.dot(np.dot(metric_values - opt_values, self.M), metric_values - opt_values))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"f1_score\", \"roc_auc_score\"]\n",
    "X, y = make_classification()\n",
    "cf = ClassificationCostFunction(metrics)\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "y_proba = model.predict_proba(X)[:, 1]\n",
    "cost = cf(y, y_proba)\n",
    "\n",
    "f1 = getattr(sm, \"f1_score\")\n",
    "roc_auc = getattr(sm, \"roc_auc_score\")\n",
    "y_pred = np.where(y_proba > 0.5, 1, 0)\n",
    "scorer_output = np.sqrt((f1(y, y_pred) - 1.0)**2 + (roc_auc(y, y_proba) - 1.0)**2)\n",
    "\n",
    "assert np.isclose(cost, scorer_output), f\"{cost} != {scorer_output}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_C</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.009353</td>\n",
       "      <td>0.003661</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>0.002612</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'C': 0.5}</td>\n",
       "      <td>-1.732076</td>\n",
       "      <td>-6.922296</td>\n",
       "      <td>-1.732076</td>\n",
       "      <td>-3.464335</td>\n",
       "      <td>-3.461615</td>\n",
       "      <td>-3.462480</td>\n",
       "      <td>1.895201</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.006416</td>\n",
       "      <td>0.000833</td>\n",
       "      <td>0.006427</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>1</td>\n",
       "      <td>{'C': 1}</td>\n",
       "      <td>-1.732076</td>\n",
       "      <td>-8.654072</td>\n",
       "      <td>-1.732076</td>\n",
       "      <td>-3.464335</td>\n",
       "      <td>-3.461615</td>\n",
       "      <td>-3.808835</td>\n",
       "      <td>2.543282</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n",
       "0       0.009353      0.003661         0.008929        0.002612     0.5   \n",
       "1       0.006416      0.000833         0.006427        0.000340       1   \n",
       "\n",
       "       params  split0_test_score  split1_test_score  split2_test_score  \\\n",
       "0  {'C': 0.5}          -1.732076          -6.922296          -1.732076   \n",
       "1    {'C': 1}          -1.732076          -8.654072          -1.732076   \n",
       "\n",
       "   split3_test_score  split4_test_score  mean_test_score  std_test_score  \\\n",
       "0          -3.464335          -3.461615        -3.462480        1.895201   \n",
       "1          -3.464335          -3.461615        -3.808835        2.543282   \n",
       "\n",
       "   rank_test_score  \n",
       "0                1  \n",
       "1                2  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = [\n",
    "        \"accuracy_score\",\n",
    "        \"f1_score\",\n",
    "        \"log_loss\",\n",
    "        \"precision_score\",\n",
    "        \"recall_score\",\n",
    "        \"roc_auc_score\"\n",
    "]\n",
    "\n",
    "param_grid = {\"C\": [0.5, 1]}\n",
    "\n",
    "scorer = ClassificationCostFunction(metrics, proba_threshold=0.5)\n",
    "cv = GridSearchCV(LogisticRegression(), param_grid, scoring=scorer.make_scorer())\n",
    "\n",
    "X, y = make_classification()\n",
    "cv.fit(X, y)\n",
    "pd.DataFrame.from_dict(cv.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
